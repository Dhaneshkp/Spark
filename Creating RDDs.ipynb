{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhaneshkp/Spark/blob/main/Creating%20RDDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMxbfLdOOXR_"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "<center> <h1> Creating RDDs </h1> </center>\n",
        "\n",
        "***`Resilient Distributed Dataset`*** or ***`RDD`*** is the Sparkâ€™s main abstraction. In this notebook we will see how to create the RDDs.\n",
        "\n",
        "\n",
        "We can create RDD in the following 2 ways:\n",
        "\n",
        "* 1. **Parallelizing existing collections**\n",
        "* 2. **Referencing a Dataset**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### `IMPORTING THE REQUIRED LIBRARIES`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dZiaAgHPOXSB",
        "outputId": "c885ede4-41d9-4f4a-8423-f7c3cda84922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# command to create sparkContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZEIkbTLOXSC"
      },
      "source": [
        "---\n",
        "\n",
        "#### `1. Parallelizing existing collections`\n",
        "\n",
        "![](https://github.com/Dhaneshkp/Spark/blob/main/images/sc_collection.png?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "psu16ROIOXSD"
      },
      "outputs": [],
      "source": [
        "# Create a list\n",
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BgASwjMJOXSD",
        "outputId": "9a5a6ef8-eabc-4e30-de63-c8c773b79e44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Type of my_list\n",
        "type(my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yxm55o4_OXSD"
      },
      "outputs": [],
      "source": [
        "# Create rdd using parallelize funciton\n",
        "rdd_list = sc.parallelize(my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fEkX4reOXSD",
        "outputId": "16b0beaa-cc00-4350-ebfb-1363c18ff246"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Type of rdd_list\n",
        "type(rdd_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YlEY9TOXSE"
      },
      "source": [
        "####  `Check number of partitions by deafult.`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XR_LzVgjOXSE",
        "outputId": "3dd1e6ce-650a-4179-c96b-482cf1daa143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Check number of partitions\n",
        "rdd_list.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wo5ypWduOXSE",
        "outputId": "ce23c761-5b95-4c2e-e72e-f9b4388e4540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Stop the existing SparkContext\n",
        "sc.stop()\n",
        "\n",
        "# Recreate the SparkContext with increased timeout settings\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Increase the timeout for the Python worker and configure Spark settings\n",
        "conf = SparkConf() \\\n",
        "    .set(\"spark.python.worker.reuse\", \"true\") \\\n",
        "    .set(\"spark.python.worker.timeout\", \"600\") \\\n",
        "    .set(\"spark.network.timeout\", \"800s\") \\\n",
        "    .set(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .set(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .set(\"spark.executor.memory\", \"4g\") \\\n",
        "    .set(\"spark.driver.memory\", \"4g\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Check data inside each partition\n",
        "rdd_list = sc.parallelize(my_list)\n",
        "rdd_list.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_hSO_FIOXSE"
      },
      "source": [
        "#### `Define the number of partitions`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iike_HO2OXSE"
      },
      "outputs": [],
      "source": [
        "# Create a list\n",
        "country_list = [\"India\", \"USA\", \"South Africa\", \"Australia\", \"France\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ddMOP0CuOXSF"
      },
      "outputs": [],
      "source": [
        "# Create rdd\n",
        "rdd_country = sc.parallelize(country_list, numSlices=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ct2gh6v7OXSF",
        "outputId": "73104a54-1f31-46ea-a3a9-7dc0a022fd13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Check number of partitions\n",
        "rdd_country.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "arQCjV8SOXSF",
        "outputId": "0fa9bb41-037f-44be-af65-ca407fdf5a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['India', 'USA'], ['South Africa', 'Australia', 'France']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Check inside each partition\n",
        "rdd_country.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDOisMyQOXSF"
      },
      "source": [
        "####  `2. External dataset`\n",
        "\n",
        "\n",
        "![](https://github.com/Dhaneshkp/Spark/blob/main/images/sc_external_dataset.png?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Txhj98tROXSF"
      },
      "outputs": [],
      "source": [
        "# Create rdd from external file\n",
        "rdd_movies = sc.textFile(\"movies.csv\", minPartitions=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rCq7bsD_OXSF",
        "outputId": "57f2c924-f3ab-4e94-8eb4-6d5207e629cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Check number of partitions\n",
        "rdd_movies.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AzVCTXNOXSF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "CancerRisk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}