{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<center> <h1> Transformations & Actions on RDDs </h1> </center>\n",
    "\n",
    "In the notebook, we will implement different transformations and actions using Python.\n",
    "\n",
    "\n",
    "\n",
    "### `Transformations` \n",
    "\n",
    "\n",
    "We will do the following transformations in this notebook:\n",
    "\n",
    "* 1. **Map**\n",
    "* 2. **FlatMap**\n",
    "* 3. **Filter**\n",
    "* 4. **Distinct**\n",
    "* 5. **Union**\n",
    "* 6. **Intersection**\n",
    "\n",
    "\n",
    "### `Actions` \n",
    "\n",
    "\n",
    "We will do the following Actions in this notebook:\n",
    "\n",
    "* 1. **Collect**\n",
    "* 2. **Take**\n",
    "* 3. **Count**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### `IMPORTING THE REQUIRED LIBRARIES`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CLTLGM0AQCVL.eu.scor.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### ` Problem Statement`\n",
    "\n",
    "Suppose there is an orgranization name **`Analytics 20`**. It has 2 different branches, one in **`India`** and another one is in **`Dubai`** We have generated a random data of the employees of this organization. One file **`analytics_20_india.txt`** contains the data of employees of India and another one **`analytics_20_dubai.txt`** that contains the data of employees of Dubai. \n",
    "\n",
    "Each line of the both the files contains 3 columns. First one is `Name of the employee`, second one is `Department Name` in which he/she works and last one is `Place Name` to which the employee belongs. Data is as shown below - \n",
    "\n",
    "<center><img src=\"images/rdd_op_dataset.png\"></center>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "#### `Reading the file - analytics_20_india.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "analytics_india = sc.textFile(\"data/analytics_20_india.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the file\n",
    "type(analytics_india) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Once we read the file in the spark, it has been converted into an RDD. \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "#### `Action - collect` \n",
    "\n",
    "**collect** action will return the complete output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/u013709/AnalyticsVidhya/Spark_Course_Handouts210719184249-220922-124120/Spark_Course_Handouts/Module 6_ RDDs in Spark/6.5_Implementation_RDD_Operations/6.5_Implementation_RDD_Operations/data/analytics_20_india.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/u013709/AnalyticsVidhya/Spark_Course_Handouts210719184249-220922-124120/Spark_Course_Handouts/Module 6_ RDDs in Spark/6.5_Implementation_RDD_Operations/6.5_Implementation_RDD_Operations/data/analytics_20_india.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# collect all records\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43manalytics_india\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\u013709\\.conda\\envs\\CancerRisk\\lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\u013709\\.conda\\envs\\CancerRisk\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\u013709\\.conda\\envs\\CancerRisk\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/u013709/AnalyticsVidhya/Spark_Course_Handouts210719184249-220922-124120/Spark_Course_Handouts/Module 6_ RDDs in Spark/6.5_Implementation_RDD_Operations/6.5_Implementation_RDD_Operations/data/analytics_20_india.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Input path does not exist: file:/c:/Users/u013709/AnalyticsVidhya/Spark_Course_Handouts210719184249-220922-124120/Spark_Course_Handouts/Module 6_ RDDs in Spark/6.5_Implementation_RDD_Operations/6.5_Implementation_RDD_Operations/data/analytics_20_india.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "# collect all records\n",
    "analytics_india.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Action - take`\n",
    "\n",
    "**take** action will return the top n (takes an integer as a parameter) results of the query. It the similar to the head funciton of pandas.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take 5 records\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Transformation - map`\n",
    "\n",
    "**map** transformation does the same operation on each of the object. \n",
    "\n",
    "We will split each line into a list of words using **map**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map transformation - tokenize records\n",
    "analytics_india_map = analytics_india.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 results\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "#### `Transformation - distinct`\n",
    "\n",
    "**distinct** is used to find the unique elements in the RDD.\n",
    "\n",
    "Find out the list of unique places of origin of the employees in the India branch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original map data\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd with only country value\n",
    "analytics_india_places = analytics_india_map.map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the distinct transformation\n",
    "analytics_india_distinct_places = analytics_india_places.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', 'Australia', 'Dubai', 'Scotland']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct country of people working in India branch\n",
    "analytics_india_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use count action to find out the total places\n",
    "analytics_india_distinct_places.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - filter`\n",
    "\n",
    "**filter** transformation only returns the elements which satisfies the given condition. \n",
    "\n",
    "Find out the data of the people who belong to the **India**.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original map data\n",
    "analytics_india_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the people who belong to the India\n",
    "analytics_india_employee_india = analytics_india_map.filter(lambda x: x[2] == \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India'],\n",
       " ['Audrey', 'Data_Science', 'India'],\n",
       " ['Tatum', 'HR', 'India'],\n",
       " ['Acton', 'Data_Science', 'India'],\n",
       " ['Ainsley', 'Data_Science', 'India'],\n",
       " ['Phillip', 'Data_Science', 'India'],\n",
       " ['Maite', 'Marketing', 'India'],\n",
       " ['Vielka', 'HR', 'India'],\n",
       " ['Risa', 'Operations', 'India'],\n",
       " ['Erich', 'Data_Science', 'India'],\n",
       " ['Francesca', 'Data_Science', 'India'],\n",
       " ['Ross', 'Sales', 'India'],\n",
       " ['Lev', 'HR', 'India'],\n",
       " ['Nerea', 'Accounts', 'India'],\n",
       " ['Halla', 'Sales', 'India'],\n",
       " ['Daquan', 'Legal', 'India'],\n",
       " ['Ivan', 'HR', 'India'],\n",
       " ['Venus', 'HR', 'India'],\n",
       " ['Lareina', 'Legal', 'India'],\n",
       " ['Denise', 'Accounts', 'India'],\n",
       " ['Sigourney', 'Legal', 'India'],\n",
       " ['Todd', 'Sales', 'India'],\n",
       " ['Jerome', 'Sales', 'India'],\n",
       " ['Signe', 'HR', 'India'],\n",
       " ['Xavier', 'Legal', 'India'],\n",
       " ['Kevin', 'Customer_Support', 'India'],\n",
       " ['Michelle', 'Customer_Support', 'India'],\n",
       " ['Ignacia', 'Customer_Support', 'India'],\n",
       " ['Lenore', 'HR', 'India']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect result\n",
    "analytics_india_employee_india.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's find out the data of the people who belongs to **Dubai** and are in **HR** department.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter transformation\n",
    "analytics_india_filtered_data = analytics_india_map.filter(lambda x: (x[1] == \"HR\") & (x[2] == \"Dubai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Irma', 'HR', 'Dubai'], ['Tarik', 'HR', 'Dubai']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results\n",
    "analytics_india_filtered_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - flatmap`\n",
    "\n",
    "* We saw **map** function does a one-to-one transformation. \n",
    "> * It transforms each element of a collection into one element of the resulting collection. \n",
    "\n",
    "<center><img src =\"images/map_transformation.png\"></center>\n",
    "\n",
    "* In the **flatmap** transformation, we will see that instead of multiple lists of each line it will return a single list of output. \n",
    "> * Spark **flatMap** function expresses a one-to-many transformation.\n",
    "\n",
    "Let's see the difference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton Data_Science India',\n",
       " 'Idona Data_Science Australia',\n",
       " 'Janna HR India',\n",
       " 'Damon Data_Science India',\n",
       " 'Rahim Marketing India']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data\n",
    "analytics_india.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatmap tranformation - tokenize records\n",
    "analytics_india_flatmap = analytics_india.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keaton',\n",
       " 'Data_Science',\n",
       " 'India',\n",
       " 'Idona',\n",
       " 'Data_Science',\n",
       " 'Australia',\n",
       " 'Janna',\n",
       " 'HR',\n",
       " 'India',\n",
       " 'Damon']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap tokenize\n",
    "analytics_india_flatmap.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Keaton', 'Data_Science', 'India'],\n",
       " ['Idona', 'Data_Science', 'Australia'],\n",
       " ['Janna', 'HR', 'India'],\n",
       " ['Damon', 'Data_Science', 'India'],\n",
       " ['Rahim', 'Marketing', 'India'],\n",
       " ['Audrey', 'Data_Science', 'India'],\n",
       " ['Irma', 'HR', 'Dubai'],\n",
       " ['Tatum', 'HR', 'India'],\n",
       " ['Acton', 'Data_Science', 'India'],\n",
       " ['Ainsley', 'Data_Science', 'India']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map tokenize\n",
    "analytics_india_map.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - union`\n",
    "\n",
    "Use **union** transformation to find out all the places of origin from both branches - India and Dubai.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd for Dubai branch file\n",
    "analytics_dubai = sc.textFile(\"data/analytics_20_dubai.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leo Customer_Support Scotland',\n",
       " 'Cyrus Customer_Support India',\n",
       " 'Jolie Sales India',\n",
       " 'Susan HR Australia',\n",
       " 'Azalia Customer_Support Dubai']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dubai data\n",
    "analytics_dubai.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tranformation - tokenize records\n",
    "analytics_dubai_places = analytics_dubai.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Leo', 'Customer_Support', 'Scotland'],\n",
       " ['Cyrus', 'Customer_Support', 'India'],\n",
       " ['Jolie', 'Sales', 'India'],\n",
       " ['Susan', 'HR', 'Australia'],\n",
       " ['Azalia', 'Customer_Support', 'Dubai']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dubai tokenized records\n",
    "analytics_dubai_places.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select distinct places from Dubai branch\n",
    "\n",
    "# Select country from each record\n",
    "analytics_dubai_places = analytics_dubai_places.map(lambda x: x[2])\n",
    "\n",
    "# Get distinct places\n",
    "analytics_dubai_distinct_places = analytics_dubai_places.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scotland', 'India', 'Australia', 'Dubai', 'South_Africa']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique places from Dubai branch\n",
    "analytics_dubai_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India', 'Australia', 'Dubai', 'Scotland']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique places from India  branch\n",
    "analytics_india_distinct_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union places from two branches\n",
    "union_places = analytics_india_distinct_places.union(analytics_dubai_distinct_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'Australia',\n",
       " 'Dubai',\n",
       " 'Scotland',\n",
       " 'Scotland',\n",
       " 'India',\n",
       " 'Australia',\n",
       " 'Dubai',\n",
       " 'South_Africa']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_places.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### `Transformation - intersection`\n",
    "\n",
    "Use **intersection** transformation to find out the common places of origin of the employees from both branches - India and Dubai. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itersection of both RDDs of unique places\n",
    "common_places = analytics_india_distinct_places.intersection(analytics_dubai_distinct_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dubai', 'India', 'Australia', 'Scotland']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the results\n",
    "common_places.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CancerRisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
